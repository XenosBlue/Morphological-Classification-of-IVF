{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQZiBxzNWrN_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import InceptionV3\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "0ZEDbe7RP5sS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTa9mwHwWs1f",
        "outputId": "360b2807-08ab-4bde-d4b2-4bbc6b7c63ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/\n",
            "/gdrive/MyDrive/IVF/FETAL/DAY 3\n",
            "\u001b[0m\u001b[01;34mgood\u001b[0m/  \u001b[01;34mpoor\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "#drive.mount('/gdrive')\n",
        " \n",
        "drive.mount('/gdrive')  \n",
        "%cd /\n",
        "%cd 'gdrive/MyDrive/IVF/FETAL/DAY 3'\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8nsuiUCWtFP"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "IMG_SIZE = 299\n",
        "NUM_CLASSES = 2\n",
        "train_dir = 'D3_Transfer'\n",
        "positive_dir = 'Do_Not_Look_Inside/p'\n",
        "negative_dir = 'Do_Not_Look_Inside/n'\n",
        "validation_dir = 'stork_set'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V6Yw4r_lQsD"
      },
      "outputs": [],
      "source": [
        "datagen = ImageDataGenerator(rescale = 1./255.,\n",
        "                                   #rotation_range = 360,\n",
        "                                   #width_shift_range = 0.2,\n",
        "                                   #height_shift_range = 0.2,\n",
        "                                   #shear_range = 0.2,\n",
        "                                   #zoom_range = 0.2,\n",
        "                                   horizontal_flip = True,\n",
        "                                   vertical_flip = True,\n",
        "                                   validation_split=0.2\n",
        "                                   )\n",
        "\n",
        "\n",
        "\n",
        "casual_generator = datagen.flow_from_directory(train_dir,\n",
        "                                                  batch_size = BATCH_SIZE,\n",
        "                                                  class_mode = 'categorical', \n",
        "                                                  target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                                  subset='training')\n",
        "\n",
        "positive_generator = datagen.flow_from_directory(positive_dir,\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    class_mode = 'categorical', \n",
        "                                                    target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                                    subset='training')\n",
        "\n",
        "negative_generator = datagen.flow_from_directory(negative_dir,\n",
        "                                                    batch_size = BATCH_SIZE,\n",
        "                                                    class_mode = 'categorical', \n",
        "                                                    target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                                    subset='training')\n",
        "\n",
        "validation_generator = datagen.flow_from_directory(train_dir,\n",
        "                                                  batch_size = BATCH_SIZE,\n",
        "                                                  class_mode = 'categorical', \n",
        "                                                  target_size = (IMG_SIZE, IMG_SIZE),\n",
        "                                                  subset='validation')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dk9WIiQLwEr"
      },
      "outputs": [],
      "source": [
        "def generator_three_img():\n",
        "\n",
        "    while True:\n",
        "        X1i = casual_generator.next()\n",
        "        X2i = positive_generator.next()\n",
        "        X3i = negative_generator.next()\n",
        "\n",
        "        Xp1 = positive_generator.next()\n",
        "        Xn1 = negative_generator.next()\n",
        "        Xp2 = positive_generator.next()\n",
        "        Xn2 = negative_generator.next()\n",
        "        Xp3 = positive_generator.next()\n",
        "        Xn3 = negative_generator.next()\n",
        "\n",
        "        y = X1i[1]\n",
        "        \n",
        "        if (Xp1[0].shape[0]!=BATCH_SIZE) or (Xn1[0].shape[0]!=BATCH_SIZE):\n",
        "          #print('Error:',X1i[0].shape[0],X2i[0].shape[0],X3i[0].shape[0])\n",
        "          continue\n",
        "\n",
        "        if (Xp2[0].shape[0]!=BATCH_SIZE) or (Xn2[0].shape[0]!=BATCH_SIZE):\n",
        "          #print('Error:',X1i[0].shape[0],X2i[0].shape[0],X3i[0].shape[0])\n",
        "          continue\n",
        "\n",
        "        if (Xp3[0].shape[0]!=BATCH_SIZE) or (Xn3[0].shape[0]!=BATCH_SIZE):\n",
        "          #print('Error:',X1i[0].shape[0],X2i[0].shape[0],X3i[0].shape[0])\n",
        "          continue\n",
        "\n",
        "        if (X1i[0].shape[0]!=BATCH_SIZE) or (X2i[0].shape[0]!=BATCH_SIZE) or (X3i[0].shape[0]!=BATCH_SIZE):\n",
        "          #print('Error:',X1i[0].shape[0],X2i[0].shape[0],X3i[0].shape[0])\n",
        "          continue\n",
        "\n",
        "        yield [X1i[0], X2i[0], X3i[0], Xp1[0], Xn1[0], Xp2[0], Xn2[0], Xp3[0], Xn3[0]], y\n",
        "\n",
        "\n",
        "# Finally create generator\n",
        "gen_flow = generator_three_img()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "5x61C0qSXF31",
        "outputId": "ef1640ce-2825-48e6-b94b-a4e71fe8b729"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 299, 299, 3) (4, 299, 299, 3) (4, 299, 299, 3)\n",
            "Example of the augmented dataset\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprint('Example of the non-augmented dataset')  \\nx,y = validation_generator.next()\\nfor i in range(1):\\n    ax = plt.subplot(2, 4, i + 1)\\n    plt.axis('off')\\n    image = x[i]\\n    plt.imshow(image)\\nplt.show()\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "x,y = next(gen_flow)\n",
        "print(x[1].shape,x[0].shape,x[2].shape)\n",
        "print('Example of the augmented dataset')\n",
        "for i in range(3):\n",
        "    ax = plt.subplot(2, 4, i + 1)\n",
        "    plt.axis('off')\n",
        "    image = x[i][0]\n",
        "    plt.imshow(image)\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "print('Example of the non-augmented dataset')  \n",
        "x,y = validation_generator.next()\n",
        "for i in range(1):\n",
        "    ax = plt.subplot(2, 4, i + 1)\n",
        "    plt.axis('off')\n",
        "    image = x[i]\n",
        "    plt.imshow(image)\n",
        "plt.show()\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Definitions"
      ],
      "metadata": {
        "id": "NgSs81XHP_ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAbQbwqaWs-M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sys\n",
        "import warnings\n",
        "# Keras Core\n",
        "from keras.layers.convolutional import MaxPooling2D, Convolution2D, AveragePooling2D\n",
        "from keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers.merge import concatenate\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras.models import Model\n",
        "# Backend\n",
        "from keras import backend as K\n",
        "# Utils\n",
        "from keras.utils.data_utils import get_file\n",
        "from tensorflow.keras.utils import get_custom_objects\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_input(x):\n",
        "    x = np.divide(x, 255.0)\n",
        "    x = np.subtract(x, 0.5)\n",
        "    x = np.multiply(x, 2.0)\n",
        "    return x\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "  alpha = 1.\n",
        "  beta  = 1.\n",
        "  return (K.sigmoid(beta * x) * alpha *x)\n",
        "\n",
        "get_custom_objects().update({'swish': swish})\n",
        "\n",
        "\n",
        "def mish(x):\n",
        "  return (K.tanh( K.softplus( x ) ) * x)\n",
        "\n",
        "get_custom_objects().update({'mish': mish})\n",
        "\n",
        "class MCDropout(Dropout):\n",
        "  def call(self, inputs):\n",
        "    return super().call(inputs, training=True)\n",
        "\n",
        "'''\n",
        "def conv2d_bn(x, nb_filter, num_row, num_col,\n",
        "              padding='same', strides=(1, 1), use_bias=False):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "    x = Convolution2D(nb_filter, (num_row, num_col),\n",
        "                      strides=strides,\n",
        "                      padding=padding,\n",
        "                      use_bias=use_bias,\n",
        "                      kernel_regularizer=regularizers.l2(0.00004),\n",
        "                      kernel_initializer=initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None))(x)\n",
        "    x = BatchNormalization(axis=channel_axis, momentum=0.9997, scale=False)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    return x\n",
        "'''\n",
        "\n",
        "w_scale = 1\n",
        "\n",
        "\n",
        "def conv2d_bn(x, nb_filter, num_row, num_col, padding='same', strides=(1, 1), use_bias=False):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "    x = Convolution2D((nb_filter*w_scale)//1, (num_row, num_col),\n",
        "                      strides=strides,\n",
        "                      padding=padding,\n",
        "                      use_bias=use_bias,\n",
        "                      #kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.00001, l2=0.001),\n",
        "                      kernel_initializer=initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='normal', seed=None))(x)\n",
        "    #x = BatchNormalization(axis=channel_axis, momentum=0.9, scale=False)(x)\n",
        "    x = Activation('mish')(x)\n",
        "    #x = MCDropout(0.3)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def block_inception_a(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 96, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 64, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 96, 3, 3)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 64, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
        "    branch_2 = conv2d_bn(branch_2, 96, 3, 3)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 96, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "\n",
        "def block_reduction_a(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 384, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 224, 3, 3)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_2 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(input)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "\n",
        "def block_inception_b(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 384, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 224, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 7, 1)\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 192, 7, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 224, 1, 7)\n",
        "    branch_2 = conv2d_bn(branch_2, 224, 7, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 256, 1, 7)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3,3), strides=(1,1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 128, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "\n",
        "def block_reduction_b(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 192, 1, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 192, 3, 3, strides=(2, 2), padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 256, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 256, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 320, 7, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 320, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    branch_2 = MaxPooling2D((3, 3), strides=(2, 2), padding='valid')(input)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "\n",
        "def block_inception_c(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    branch_0 = conv2d_bn(input, 256, 1, 1)\n",
        "\n",
        "    branch_1 = conv2d_bn(input, 384, 1, 1)\n",
        "    branch_10 = conv2d_bn(branch_1, 256, 1, 3)\n",
        "    branch_11 = conv2d_bn(branch_1, 256, 3, 1)\n",
        "    branch_1 = concatenate([branch_10, branch_11], axis=channel_axis)\n",
        "\n",
        "\n",
        "    branch_2 = conv2d_bn(input, 384, 1, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 448, 3, 1)\n",
        "    branch_2 = conv2d_bn(branch_2, 512, 1, 3)\n",
        "    branch_20 = conv2d_bn(branch_2, 256, 1, 3)\n",
        "    branch_21 = conv2d_bn(branch_2, 256, 3, 1)\n",
        "    branch_2 = concatenate([branch_20, branch_21], axis=channel_axis)\n",
        "\n",
        "    branch_3 = AveragePooling2D((3, 3), strides=(1, 1), padding='same')(input)\n",
        "    branch_3 = conv2d_bn(branch_3, 256, 1, 1)\n",
        "\n",
        "    x = concatenate([branch_0, branch_1, branch_2, branch_3], axis=channel_axis)\n",
        "    return x\n",
        "\n",
        "\n",
        "def inception_v4_base(input):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        channel_axis = 1\n",
        "    else:\n",
        "        channel_axis = -1\n",
        "\n",
        "    # Input Shape is 299 x 299 x 3 (th) or 3 x 299 x 299 (th)\n",
        "    net = conv2d_bn(input, 32, 3, 3, strides=(2,2), padding='valid')\n",
        "    net = conv2d_bn(net, 32, 3, 3, padding='valid')\n",
        "    net = conv2d_bn(net, 64, 3, 3)\n",
        "\n",
        "    branch_0 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
        "\n",
        "    branch_1 = conv2d_bn(net, 96, 3, 3, strides=(2,2), padding='valid')\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    branch_0 = conv2d_bn(net, 64, 1, 1)\n",
        "    branch_0 = conv2d_bn(branch_0, 96, 3, 3, padding='valid')\n",
        "\n",
        "    branch_1 = conv2d_bn(net, 64, 1, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 1, 7)\n",
        "    branch_1 = conv2d_bn(branch_1, 64, 7, 1)\n",
        "    branch_1 = conv2d_bn(branch_1, 96, 3, 3, padding='valid')\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    branch_0 = conv2d_bn(net, 192, 3, 3, strides=(2,2), padding='valid')\n",
        "    branch_1 = MaxPooling2D((3,3), strides=(2,2), padding='valid')(net)\n",
        "\n",
        "    net = concatenate([branch_0, branch_1], axis=channel_axis)\n",
        "\n",
        "    # 35 x 35 x 384\n",
        "    # 4 x Inception-A blocks\n",
        "    for idx in range(4):\n",
        "    \tnet = block_inception_a(net)\n",
        "\n",
        "    # 35 x 35 x 384\n",
        "    # Reduction-A block\n",
        "    net = block_reduction_a(net)\n",
        "\n",
        "    # 17 x 17 x 1024\n",
        "    # 7 x Inception-B blocks\n",
        "    for idx in range(7):\n",
        "    \tnet = block_inception_b(net)\n",
        "\n",
        "    # 17 x 17 x 1024\n",
        "    # Reduction-B block\n",
        "    net = block_reduction_b(net)\n",
        "\n",
        "    # 8 x 8 x 1536\n",
        "    # 3 x Inception-C blocks\n",
        "    for idx in range(3):\n",
        "    \tnet = block_inception_c(net)\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def inception_v4(num_classes, dropout_keep_prob, weights, include_top):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        inputs = Input((3, 299, 299))\n",
        "    else:\n",
        "        inputs = Input((299, 299, 3))\n",
        "    x = inception_v4_base(inputs)\n",
        "\n",
        "    if include_top:\n",
        "        # 1 x 1 x 1536\n",
        "        x = AveragePooling2D((8,8), padding='valid')(x)\n",
        "        #x = MCDropout(dropout_keep_prob)(x)\n",
        "        #x = Flatten()(x)\n",
        "        # 1536\n",
        "        '''\n",
        "        y = Dense(units=(1536*w_scale)//1, activation=None)(x)\n",
        "        y = Activation('mish')(y)\n",
        "        y = MCDropout(dropout_keep_prob)(y)\n",
        "        x = layers.Add()([x,y])\n",
        "        '''\n",
        "\n",
        "    model = Model(inputs, x, name='inception_v4')\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model(num_classes=1001, dropout_prob=0.2, weights=None, include_top=True):\n",
        "  model = inception_v4(num_classes, dropout_prob, weights, include_top)\n",
        "  #model.summary()\n",
        "  return model\n",
        "\n",
        "\n",
        "class DistanceLayer(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def call(self, casual, positive, negative, positive1, negative1, positive2, negative2, positive3, negative3):\n",
        "\n",
        "        \n",
        "        positive_distance = tf.reduce_sum(tf.square(casual - positive), -1)\n",
        "        negative_distance = tf.reduce_sum(tf.square(casual - negative), -1)\n",
        "        positive_distance = layers.Reshape((1,1))(positive_distance)\n",
        "        negative_distance = layers.Reshape((1,1))(negative_distance)\n",
        "\n",
        "        positive_distance1 = tf.reduce_sum(tf.square(casual - positive1), -1)\n",
        "        negative_distance1 = tf.reduce_sum(tf.square(casual - negative1), -1)\n",
        "        positive_distance1 = layers.Reshape((1,1))(positive_distance1)\n",
        "        negative_distance1 = layers.Reshape((1,1))(negative_distance1)\n",
        "\n",
        "        positive_distance2 = tf.reduce_sum(tf.square(casual - positive2), -1)\n",
        "        negative_distance2 = tf.reduce_sum(tf.square(casual - negative2), -1)\n",
        "        positive_distance2 = layers.Reshape((1,1))(positive_distance2)\n",
        "        negative_distance2 = layers.Reshape((1,1))(negative_distance2)\n",
        "\n",
        "        positive_distance3 = tf.reduce_sum(tf.square(casual - positive3), -1)\n",
        "        negative_distance3 = tf.reduce_sum(tf.square(casual - negative3), -1)\n",
        "        positive_distance3 = layers.Reshape((1,1))(positive_distance3)\n",
        "        negative_distance3 = layers.Reshape((1,1))(negative_distance3)\n",
        "        \n",
        "        positive_euclid = positive_distance + positive_distance1 + positive_distance2 + positive_distance3\n",
        "        negative_euclid = negative_distance + negative_distance1 + negative_distance2 + negative_distance3\n",
        "\n",
        "        positive_distance = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(positive, 0)),axis=-1)\n",
        "        negative_distance = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(negative, 0)),axis=-1)\n",
        "        positive_distance = layers.Reshape((1,1))(positive_distance)\n",
        "        negative_distance = layers.Reshape((1,1))(negative_distance)\n",
        "\n",
        "        positive_distance1 = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(positive1, 0)),axis=-1)\n",
        "        negative_distance1 = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(negative1, 0)),axis=-1)\n",
        "        positive_distance1 = layers.Reshape((1,1))(positive_distance1)\n",
        "        negative_distance1 = layers.Reshape((1,1))(negative_distance1)\n",
        "\n",
        "        positive_distance2 = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(positive2, 0)),axis=-1)\n",
        "        negative_distance2 = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(negative2, 0)),axis=-1)\n",
        "        positive_distance2 = layers.Reshape((1,1))(positive_distance2)\n",
        "        negative_distance2 = layers.Reshape((1,1))(negative_distance2)\n",
        "\n",
        "        positive_distance3 = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(positive3, 0)),axis=-1)\n",
        "        negative_distance3 = tf.reduce_sum(tf.multiply(tf.nn.l2_normalize(casual, 0), tf.nn.l2_normalize(negative3, 0)),axis=-1)\n",
        "        positive_distance3 = layers.Reshape((1,1))(positive_distance3)\n",
        "        negative_distance3 = layers.Reshape((1,1))(negative_distance3)\n",
        "\n",
        "\n",
        "        positive_cosine = positive_distance + positive_distance1 + positive_distance2 + positive_distance3\n",
        "        negative_cosine = negative_distance + negative_distance1 + negative_distance2 + negative_distance3\n",
        "        \n",
        "        positive_distance = positive_euclid + positive_cosine\n",
        "        negative_distance = negative_euclid + negative_cosine\n",
        "\n",
        "        return  (positive_distance/6, negative_distance/6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRizRfPfP9Vb"
      },
      "outputs": [],
      "source": [
        "casual   = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"casual\")\n",
        "positive = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"positive\")\n",
        "negative = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"negative\")\n",
        "\n",
        "\n",
        "positive1 = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"positive1\")\n",
        "negative1 = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"negative1\")\n",
        "positive2 = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"positive2\")\n",
        "negative2 = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"negative2\")\n",
        "positive3 = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"positive3\")\n",
        "negative3 = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"negative3\")\n",
        "\n",
        "encoder = create_model(num_classes = 2, dropout_prob = 0.5, weights=None, include_top = True)\n",
        "#encoder = tf.keras.applications.Xception(include_top=False,weights=\"imagenet\")\n",
        "\n",
        "encoded_casual = encoder(casual)\n",
        "encoded_positive = encoder(positive)\n",
        "encoded_negative = encoder(negative)\n",
        "\n",
        "\n",
        "encoded_positive1 = encoder(positive1)\n",
        "encoded_negative1 = encoder(negative2)\n",
        "encoded_positive2 = encoder(positive1)\n",
        "encoded_negative2 = encoder(negative2)\n",
        "encoded_positive3 = encoder(positive3)\n",
        "encoded_negative3 = encoder(negative3)\n",
        "\n",
        "#encoded_casual   = BatchNormalization(momentum=0.99)(encoded_casual)\n",
        "#encoded_positive = BatchNormalization(momentum=0.99)(encoded_positive)\n",
        "#encoded_negative = BatchNormalization(momentum=0.99)(encoded_negative)\n",
        "\n",
        "#print(encoded_casual)\n",
        "#print(encoded_positive)\n",
        "\n",
        "p,n = DistanceLayer()(\n",
        "    encoded_casual,\n",
        "    encoded_positive,\n",
        "    encoded_negative,\n",
        "\n",
        "    encoded_positive1,\n",
        "    encoded_negative1,\n",
        "    encoded_positive2,\n",
        "    encoded_negative2,\n",
        "    encoded_positive3,\n",
        "    encoded_negative3\n",
        ")\n",
        "\n",
        "\n",
        "out = concatenate([p,n], axis=-1)\n",
        "\n",
        "\n",
        "\n",
        "#print(out)\n",
        "\n",
        "model = Model(inputs = [casual,positive,negative,positive1,negative1,positive2,negative2,positive3,negative3], outputs = out, name='fuck_IVF')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGFuyMjxRjSO"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Contrastive Model Stack and Training"
      ],
      "metadata": {
        "id": "X2fJ0fI7QGma"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wM3Cpgp9wa6"
      },
      "outputs": [],
      "source": [
        "casual0   = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"casual\")\n",
        "\n",
        "encoded_casual = encoder(casual0)\n",
        "x = Flatten()(encoded_casual)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "out0 = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "classifier = Model(inputs = casual0, outputs = out0, name='fuck_IVF_with_class')\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "loss = 'categorical_crossentropy'\n",
        "#loss = tf.keras.losses.CategoricalHinge()\n",
        "metrics = ['accuracy']\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=loss, metrics=metrics#[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./model_checkpoint/modelv6.0',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8,patience=5, min_lr=0.000001),\n",
        "    #model_checkpoint_callback,\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]\n",
        "\n",
        "epochs = 524  # @param {type: \"slider\", min:1, max:1000}\n",
        "hist = classifier.fit(casual_generator,\n",
        "                 validation_data = validation_generator,\n",
        "                 epochs=epochs,\n",
        "                 #steps_per_epoch = 150//4,\n",
        "                 callbacks=my_callbacks,\n",
        "                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_LsOBGHl_uL"
      },
      "outputs": [],
      "source": [
        "def loss_fn(margin=1):\n",
        "  def triplet_loss(y_true, distances):  \n",
        "    #print(\"CHECK:\",y_true,distances)\n",
        "    #print(tf.shape(distances))\n",
        "    #print(tf.shape(y_true))\n",
        "    #y_true = tf.squeeze(y_true,[1])\n",
        "    phi = tf.constant([[1.0, -1.0]])\n",
        "    y_true = tf.reshape(y_true,[-1,2])\n",
        "    #print(tf.shape(y_true))\n",
        "    dist_loss  = tf.matmul(distances,tf.transpose(phi))\n",
        "    label_loss = tf.matmul(y_true,tf.transpose(phi))\n",
        "    #print('dist loss: ',dist_loss)\n",
        "    #print('label loss: ',label_loss)\n",
        "    loss = dist_loss * label_loss\n",
        "    #print('loss: ',loss)\n",
        "    #margin = tf.reduce_max(y_true)\n",
        "    print(y_true,margin)\n",
        "    loss = tf.maximum(loss + margin, 0.0)\n",
        "    #print('loss: ',loss)\n",
        "    return loss\n",
        "  return triplet_loss\n",
        "\n",
        "def dist_accuracy(y_true, y_pred):\n",
        "    #comparison = tf.equal( y_pred,  tf.reduce_max(y_pred)  )\n",
        "    #y_pred = tf.where( comparison, 1.0,0.0)\n",
        "    comparison, _ = tf.nn.top_k(y_pred, 1)\n",
        "    y_pred = tf.cast(tf.greater_equal(y_pred, comparison), tf.float32)\n",
        "    y_true_ex = tf.expand_dims(y_true,1)\n",
        "     #print(y_pred,y_true_ex)\n",
        "    del_y = y_true_ex - y_pred\n",
        "    y = tf.reduce_max( del_y,axis=2)\n",
        "\n",
        "    '''\n",
        "    print(\"\\n*************\\n\")\n",
        "    print(tf.shape(comparison))\n",
        "    print(tf.shape(y_pred))\n",
        "    print(tf.shape(y_true))\n",
        "    #print(tf.shape(y_true_ex))\n",
        "    print(tf.shape(del_y))\n",
        "    print(tf.shape(y))\n",
        "    print(tf.shape(tf.reduce_mean(y)))\n",
        "    print(\"\\n*************\")\n",
        "\n",
        "    \n",
        "    print(\"\\n\\n*************\\n\")\n",
        "    print(comparison)\n",
        "    print(y_pred)\n",
        "    print(y_true_ex)\n",
        "    print(y_true)\n",
        "    print(del_y)\n",
        "    print(y)\n",
        "    print(tf.reduce_mean(y))\n",
        "    print(\"*************\")\n",
        "    '''\n",
        "    \n",
        "    return tf.reduce_mean(y) \n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "\n",
        "\n",
        "#loss = 'categorical_crossentropy'\n",
        "#loss = tf.keras.losses.CategoricalHinge()\n",
        "\n",
        "\n",
        "metrics = [\n",
        "           dist_accuracy\n",
        "           #'accuracy',\n",
        "           #tf.keras.metrics.Recall(),\n",
        "           #tf.keras.metrics.Precision()\n",
        "           ]\n",
        "\n",
        "\n",
        "model.compile(\n",
        "    optimizer=optimizer, loss=loss_fn(margin=0), metrics=metrics#[\"accuracy\"]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNUXqB5AXF62"
      },
      "outputs": [],
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./model_checkpoint/modelv6.0',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8,patience=3, min_lr=0.00000001),\n",
        "    #model_checkpoint_callback,\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]\n",
        "\n",
        "epochs = 300  # @param {type: \"slider\", min:1, max:1000}\n",
        "hist = model.fit(gen_flow,\n",
        "                 #validation_data = validation_generator,\n",
        "                 epochs=epochs,\n",
        "                 steps_per_epoch = 150//BATCH_SIZE,\n",
        "                 callbacks=my_callbacks,\n",
        "                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWSgrwTBg9Ut"
      },
      "outputs": [],
      "source": [
        "check = 4\n",
        "x,y = next(gen_flow)\n",
        "p = model(x, training=False)   \n",
        "\n",
        "lossy = loss_fn(margin=0)\n",
        "l = lossy(p,y)\n",
        "print('=>',dist_accuracy(y,p))\n",
        "#print('=>',l)\n",
        "#print('=>',(p,y))\n",
        "\n",
        "p = p.numpy()\n",
        "\n",
        "for i in range(check):\n",
        "  print('=====================\\n',p[i],y[i],l[i],'\\n=====================')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-RizL72Dzpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf415d1b-a56a-49be-87c9-0e36fb9f98e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "31/31 [==============================] - 42s 1s/step - loss: 11064137.0000 - accuracy: 0.4793 - val_loss: 3241167.7500 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 2/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 11089556.0000 - accuracy: 0.4959 - val_loss: 1125791.5000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 3/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 9602218.0000 - accuracy: 0.5372 - val_loss: 15430630.0000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 4/300\n",
            "31/31 [==============================] - 3s 89ms/step - loss: 10204979.0000 - accuracy: 0.4793 - val_loss: 4787296.5000 - val_accuracy: 0.6333 - lr: 0.0100\n",
            "Epoch 5/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 12656036.0000 - accuracy: 0.5207 - val_loss: 3007614.7500 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 6/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 10826020.0000 - accuracy: 0.3967 - val_loss: 5248924.5000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 7/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 9047714.0000 - accuracy: 0.5455 - val_loss: 5439038.0000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 8/300\n",
            "31/31 [==============================] - 2s 66ms/step - loss: 9620315.0000 - accuracy: 0.5702 - val_loss: 19850450.0000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 9/300\n",
            "31/31 [==============================] - 2s 67ms/step - loss: 23627810.0000 - accuracy: 0.5041 - val_loss: 22669566.0000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 10/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 17955364.0000 - accuracy: 0.5041 - val_loss: 3392603.0000 - val_accuracy: 0.3667 - lr: 0.0100\n",
            "Epoch 11/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 9366595.0000 - accuracy: 0.5868 - val_loss: 3762527.5000 - val_accuracy: 0.6333 - lr: 0.0100\n",
            "Epoch 12/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 10398461.0000 - accuracy: 0.4959 - val_loss: 11947973.0000 - val_accuracy: 0.6333 - lr: 0.0100\n",
            "Epoch 13/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 14697970.0000 - accuracy: 0.5289 - val_loss: 12130234.0000 - val_accuracy: 0.6333 - lr: 0.0080\n",
            "Epoch 14/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 15814745.0000 - accuracy: 0.4380 - val_loss: 8644323.0000 - val_accuracy: 0.6333 - lr: 0.0080\n",
            "Epoch 15/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 10894264.0000 - accuracy: 0.4545 - val_loss: 4471634.5000 - val_accuracy: 0.3667 - lr: 0.0080\n",
            "Epoch 16/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 13698893.0000 - accuracy: 0.4463 - val_loss: 7435061.5000 - val_accuracy: 0.6333 - lr: 0.0080\n",
            "Epoch 17/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 6540697.0000 - accuracy: 0.6116 - val_loss: 11565891.0000 - val_accuracy: 0.3667 - lr: 0.0080\n",
            "Epoch 18/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 13886177.0000 - accuracy: 0.5620 - val_loss: 1408024.7500 - val_accuracy: 0.6333 - lr: 0.0080\n",
            "Epoch 19/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 9377615.0000 - accuracy: 0.5455 - val_loss: 17985066.0000 - val_accuracy: 0.6333 - lr: 0.0080\n",
            "Epoch 20/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 7906092.0000 - accuracy: 0.5455 - val_loss: 3879565.2500 - val_accuracy: 0.3667 - lr: 0.0080\n",
            "Epoch 21/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 13195026.0000 - accuracy: 0.5289 - val_loss: 11219964.0000 - val_accuracy: 0.3667 - lr: 0.0080\n",
            "Epoch 22/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 10636745.0000 - accuracy: 0.6198 - val_loss: 5531077.0000 - val_accuracy: 0.6333 - lr: 0.0080\n",
            "Epoch 23/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 12823740.0000 - accuracy: 0.5785 - val_loss: 28624026.0000 - val_accuracy: 0.6333 - lr: 0.0064\n",
            "Epoch 24/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 9885352.0000 - accuracy: 0.5537 - val_loss: 18814.6250 - val_accuracy: 0.6333 - lr: 0.0064\n",
            "Epoch 25/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 9116584.0000 - accuracy: 0.4876 - val_loss: 314655.6562 - val_accuracy: 0.6333 - lr: 0.0064\n",
            "Epoch 26/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 16272907.0000 - accuracy: 0.5620 - val_loss: 11118128.0000 - val_accuracy: 0.3667 - lr: 0.0064\n",
            "Epoch 27/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 6712718.0000 - accuracy: 0.5455 - val_loss: 2185372.2500 - val_accuracy: 0.6333 - lr: 0.0064\n",
            "Epoch 28/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 7845146.5000 - accuracy: 0.4959 - val_loss: 13348749.0000 - val_accuracy: 0.3667 - lr: 0.0051\n",
            "Epoch 29/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 18952924.0000 - accuracy: 0.4545 - val_loss: 12382854.0000 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 30/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 7751132.0000 - accuracy: 0.5702 - val_loss: 14686705.0000 - val_accuracy: 0.3667 - lr: 0.0051\n",
            "Epoch 31/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 8158582.5000 - accuracy: 0.5289 - val_loss: 1224115.5000 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 32/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 4622054.5000 - accuracy: 0.5620 - val_loss: 664316.3750 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 33/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 5263019.0000 - accuracy: 0.5124 - val_loss: 4637265.5000 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 34/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 5057033.0000 - accuracy: 0.5207 - val_loss: 2345499.5000 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 35/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 4119476.7500 - accuracy: 0.5372 - val_loss: 3307262.5000 - val_accuracy: 0.3667 - lr: 0.0051\n",
            "Epoch 36/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 10926474.0000 - accuracy: 0.5289 - val_loss: 5359811.0000 - val_accuracy: 0.3667 - lr: 0.0051\n",
            "Epoch 37/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 7525275.0000 - accuracy: 0.5041 - val_loss: 13138082.0000 - val_accuracy: 0.3667 - lr: 0.0051\n",
            "Epoch 38/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 4700441.5000 - accuracy: 0.5372 - val_loss: 6287867.5000 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 39/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 6146993.0000 - accuracy: 0.6281 - val_loss: 4551681.5000 - val_accuracy: 0.6333 - lr: 0.0051\n",
            "Epoch 40/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 5825052.5000 - accuracy: 0.5537 - val_loss: 671669.5625 - val_accuracy: 0.3667 - lr: 0.0051\n",
            "Epoch 41/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 6285922.0000 - accuracy: 0.5289 - val_loss: 1880560.2500 - val_accuracy: 0.6333 - lr: 0.0041\n",
            "Epoch 42/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 9306040.0000 - accuracy: 0.4959 - val_loss: 13741402.0000 - val_accuracy: 0.3667 - lr: 0.0041\n",
            "Epoch 43/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 5932535.0000 - accuracy: 0.5289 - val_loss: 1387730.6250 - val_accuracy: 0.6333 - lr: 0.0041\n",
            "Epoch 44/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 4719334.0000 - accuracy: 0.5702 - val_loss: 4915531.0000 - val_accuracy: 0.6333 - lr: 0.0041\n",
            "Epoch 45/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 4923800.5000 - accuracy: 0.5372 - val_loss: 6261675.5000 - val_accuracy: 0.6333 - lr: 0.0041\n",
            "Epoch 46/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 4824802.5000 - accuracy: 0.4959 - val_loss: 4970449.5000 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 47/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 4485240.5000 - accuracy: 0.5537 - val_loss: 4889608.0000 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 48/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 3920883.2500 - accuracy: 0.5207 - val_loss: 297069.2812 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 49/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 3098710.7500 - accuracy: 0.5702 - val_loss: 1908717.5000 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 50/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 3303704.5000 - accuracy: 0.5207 - val_loss: 353138.5938 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 51/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 3610376.5000 - accuracy: 0.5041 - val_loss: 639736.0000 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 52/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 3467916.5000 - accuracy: 0.5537 - val_loss: 827018.2500 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 53/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 2994956.5000 - accuracy: 0.5537 - val_loss: 1393459.1250 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 54/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 5687801.0000 - accuracy: 0.5207 - val_loss: 4116067.7500 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 55/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 5556675.0000 - accuracy: 0.5289 - val_loss: 3147730.2500 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 56/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 2795564.7500 - accuracy: 0.5785 - val_loss: 1745045.7500 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 57/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 2430426.7500 - accuracy: 0.5455 - val_loss: 4038717.0000 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 58/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 2751749.0000 - accuracy: 0.5537 - val_loss: 2190142.7500 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 59/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 2602321.7500 - accuracy: 0.5537 - val_loss: 1957388.8750 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 60/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 3239356.5000 - accuracy: 0.5702 - val_loss: 3421877.2500 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 61/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 3918988.5000 - accuracy: 0.4793 - val_loss: 11870666.0000 - val_accuracy: 0.6333 - lr: 0.0033\n",
            "Epoch 62/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 8756101.0000 - accuracy: 0.5455 - val_loss: 13045337.0000 - val_accuracy: 0.3667 - lr: 0.0033\n",
            "Epoch 63/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 5333780.0000 - accuracy: 0.5207 - val_loss: 1898222.6250 - val_accuracy: 0.6333 - lr: 0.0026\n",
            "Epoch 64/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 2312682.5000 - accuracy: 0.5537 - val_loss: 1403583.1250 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 65/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 2936446.0000 - accuracy: 0.4959 - val_loss: 2581134.2500 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 66/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 2519715.2500 - accuracy: 0.4463 - val_loss: 405249.9688 - val_accuracy: 0.6333 - lr: 0.0026\n",
            "Epoch 67/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1911784.6250 - accuracy: 0.5950 - val_loss: 2251246.5000 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 68/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 1408562.8750 - accuracy: 0.5289 - val_loss: 706598.5625 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 69/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 3130049.0000 - accuracy: 0.5207 - val_loss: 1774525.3750 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 70/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1733189.2500 - accuracy: 0.5537 - val_loss: 995679.1250 - val_accuracy: 0.6333 - lr: 0.0026\n",
            "Epoch 71/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1445271.6250 - accuracy: 0.4545 - val_loss: 327687.5312 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 72/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 2576033.0000 - accuracy: 0.5372 - val_loss: 1234835.5000 - val_accuracy: 0.3667 - lr: 0.0026\n",
            "Epoch 73/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 2250623.7500 - accuracy: 0.5868 - val_loss: 3083079.5000 - val_accuracy: 0.6333 - lr: 0.0026\n",
            "Epoch 74/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 2487585.2500 - accuracy: 0.4793 - val_loss: 1490907.6250 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 75/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 2040159.8750 - accuracy: 0.5289 - val_loss: 915212.6875 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 76/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1370618.8750 - accuracy: 0.5207 - val_loss: 701223.8125 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 77/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 2012251.5000 - accuracy: 0.5041 - val_loss: 51235.4648 - val_accuracy: 0.3667 - lr: 0.0021\n",
            "Epoch 78/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1887091.8750 - accuracy: 0.5124 - val_loss: 1269326.3750 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 79/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 2086763.5000 - accuracy: 0.5455 - val_loss: 904778.2500 - val_accuracy: 0.3667 - lr: 0.0021\n",
            "Epoch 80/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 1081926.2500 - accuracy: 0.5455 - val_loss: 202439.1875 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 81/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1423666.7500 - accuracy: 0.5785 - val_loss: 263788.4062 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 82/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1138151.3750 - accuracy: 0.6033 - val_loss: 456216.9062 - val_accuracy: 0.3667 - lr: 0.0021\n",
            "Epoch 83/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 3260928.5000 - accuracy: 0.4711 - val_loss: 1212149.6250 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 84/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1592400.5000 - accuracy: 0.5702 - val_loss: 820212.8750 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 85/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 1743382.7500 - accuracy: 0.4793 - val_loss: 1185146.8750 - val_accuracy: 0.6333 - lr: 0.0021\n",
            "Epoch 86/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1362694.7500 - accuracy: 0.5868 - val_loss: 1498902.3750 - val_accuracy: 0.3667 - lr: 0.0017\n",
            "Epoch 87/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1860813.6250 - accuracy: 0.5950 - val_loss: 1018281.1875 - val_accuracy: 0.6333 - lr: 0.0017\n",
            "Epoch 88/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1479146.5000 - accuracy: 0.5289 - val_loss: 191631.4062 - val_accuracy: 0.6333 - lr: 0.0017\n",
            "Epoch 89/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1400868.3750 - accuracy: 0.5041 - val_loss: 1064670.5000 - val_accuracy: 0.6333 - lr: 0.0017\n",
            "Epoch 90/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1288827.7500 - accuracy: 0.5455 - val_loss: 449083.3438 - val_accuracy: 0.6333 - lr: 0.0017\n",
            "Epoch 91/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1685462.2500 - accuracy: 0.5537 - val_loss: 188543.6719 - val_accuracy: 0.6333 - lr: 0.0013\n",
            "Epoch 92/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1067747.6250 - accuracy: 0.5124 - val_loss: 984808.5625 - val_accuracy: 0.3667 - lr: 0.0013\n",
            "Epoch 93/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1259068.3750 - accuracy: 0.4545 - val_loss: 929147.8750 - val_accuracy: 0.3667 - lr: 0.0013\n",
            "Epoch 94/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1534591.0000 - accuracy: 0.4876 - val_loss: 2761851.5000 - val_accuracy: 0.6333 - lr: 0.0013\n",
            "Epoch 95/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1342153.7500 - accuracy: 0.5537 - val_loss: 85512.8438 - val_accuracy: 0.3667 - lr: 0.0013\n",
            "Epoch 96/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 915084.8750 - accuracy: 0.6033 - val_loss: 159120.6094 - val_accuracy: 0.6333 - lr: 0.0013\n",
            "Epoch 97/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 905103.1875 - accuracy: 0.5537 - val_loss: 432074.6875 - val_accuracy: 0.6333 - lr: 0.0013\n",
            "Epoch 98/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 706949.7500 - accuracy: 0.6364 - val_loss: 259792.0000 - val_accuracy: 0.6333 - lr: 0.0013\n",
            "Epoch 99/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 822133.1875 - accuracy: 0.5455 - val_loss: 32832.6797 - val_accuracy: 0.3667 - lr: 0.0013\n",
            "Epoch 100/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 869245.2500 - accuracy: 0.5537 - val_loss: 1256981.7500 - val_accuracy: 0.3667 - lr: 0.0013\n",
            "Epoch 101/300\n",
            "31/31 [==============================] - 3s 108ms/step - loss: 1063205.3750 - accuracy: 0.5785 - val_loss: 1053.5643 - val_accuracy: 0.6667 - lr: 0.0013\n",
            "Epoch 102/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 813642.5000 - accuracy: 0.5537 - val_loss: 57562.7070 - val_accuracy: 0.6333 - lr: 0.0013\n",
            "Epoch 103/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1041620.6875 - accuracy: 0.5207 - val_loss: 690835.8125 - val_accuracy: 0.3667 - lr: 0.0013\n",
            "Epoch 104/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1090779.6250 - accuracy: 0.6033 - val_loss: 862853.6250 - val_accuracy: 0.3667 - lr: 0.0011\n",
            "Epoch 105/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1390002.0000 - accuracy: 0.4959 - val_loss: 1242990.6250 - val_accuracy: 0.6333 - lr: 0.0011\n",
            "Epoch 106/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 1045332.0625 - accuracy: 0.5455 - val_loss: 910566.5625 - val_accuracy: 0.6333 - lr: 0.0011\n",
            "Epoch 107/300\n",
            "31/31 [==============================] - 2s 70ms/step - loss: 1293355.7500 - accuracy: 0.5455 - val_loss: 1186645.5000 - val_accuracy: 0.3667 - lr: 0.0011\n",
            "Epoch 108/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1117073.6250 - accuracy: 0.4711 - val_loss: 771344.3125 - val_accuracy: 0.6333 - lr: 0.0011\n",
            "Epoch 109/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 998319.3750 - accuracy: 0.5455 - val_loss: 391201.1875 - val_accuracy: 0.3667 - lr: 8.5899e-04\n",
            "Epoch 110/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1033053.3125 - accuracy: 0.5289 - val_loss: 205873.5000 - val_accuracy: 0.3667 - lr: 8.5899e-04\n",
            "Epoch 111/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 1029784.6875 - accuracy: 0.5950 - val_loss: 291398.8125 - val_accuracy: 0.3667 - lr: 8.5899e-04\n",
            "Epoch 112/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 1239508.2500 - accuracy: 0.3802 - val_loss: 365909.5000 - val_accuracy: 0.6333 - lr: 8.5899e-04\n",
            "Epoch 113/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 1075894.0000 - accuracy: 0.5124 - val_loss: 766522.6875 - val_accuracy: 0.6333 - lr: 8.5899e-04\n",
            "Epoch 114/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 873769.0000 - accuracy: 0.6033 - val_loss: 87559.7734 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 115/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 612760.1250 - accuracy: 0.4876 - val_loss: 241393.7656 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 116/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 550097.7500 - accuracy: 0.5785 - val_loss: 36512.1367 - val_accuracy: 0.3667 - lr: 6.8719e-04\n",
            "Epoch 117/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 572545.3125 - accuracy: 0.5289 - val_loss: 122283.9531 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 118/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 676805.5625 - accuracy: 0.5372 - val_loss: 392656.7188 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 119/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 630566.0625 - accuracy: 0.4793 - val_loss: 137457.5781 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 120/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 634791.3750 - accuracy: 0.5041 - val_loss: 398226.6875 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 121/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 507422.3125 - accuracy: 0.4876 - val_loss: 283534.0625 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 122/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 836142.5625 - accuracy: 0.6033 - val_loss: 977792.3125 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 123/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 573851.8750 - accuracy: 0.4959 - val_loss: 482998.0000 - val_accuracy: 0.3667 - lr: 6.8719e-04\n",
            "Epoch 124/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 536197.7500 - accuracy: 0.5372 - val_loss: 12715.8525 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 125/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 508335.5625 - accuracy: 0.5289 - val_loss: 88925.5391 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 126/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 409078.3438 - accuracy: 0.6033 - val_loss: 611370.2500 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 127/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 597067.8750 - accuracy: 0.6116 - val_loss: 1205119.8750 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 128/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 686626.3750 - accuracy: 0.4793 - val_loss: 110936.1953 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 129/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 563704.7500 - accuracy: 0.5124 - val_loss: 79652.3516 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 130/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 550096.6875 - accuracy: 0.5620 - val_loss: 503608.8750 - val_accuracy: 0.6333 - lr: 6.8719e-04\n",
            "Epoch 131/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 990866.0625 - accuracy: 0.4876 - val_loss: 117542.6641 - val_accuracy: 0.3667 - lr: 6.8719e-04\n",
            "Epoch 132/300\n",
            "31/31 [==============================] - 2s 68ms/step - loss: 846993.0000 - accuracy: 0.4959 - val_loss: 1014962.5625 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 133/300\n",
            "31/31 [==============================] - 2s 65ms/step - loss: 834545.0625 - accuracy: 0.5372 - val_loss: 158407.4062 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 134/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 496864.1875 - accuracy: 0.5041 - val_loss: 114238.7500 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 135/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 510568.6875 - accuracy: 0.5207 - val_loss: 30801.1543 - val_accuracy: 0.3667 - lr: 5.4976e-04\n",
            "Epoch 136/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 384305.8438 - accuracy: 0.5785 - val_loss: 34119.8320 - val_accuracy: 0.3667 - lr: 5.4976e-04\n",
            "Epoch 137/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 534400.2500 - accuracy: 0.5455 - val_loss: 212520.1406 - val_accuracy: 0.3667 - lr: 5.4976e-04\n",
            "Epoch 138/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 440527.1875 - accuracy: 0.5041 - val_loss: 76961.6719 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 139/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 448910.9375 - accuracy: 0.5537 - val_loss: 21521.9922 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 140/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 551596.3125 - accuracy: 0.5289 - val_loss: 236938.3594 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 141/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 556977.5000 - accuracy: 0.5455 - val_loss: 294814.8750 - val_accuracy: 0.6333 - lr: 5.4976e-04\n",
            "Epoch 142/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 399405.4688 - accuracy: 0.5537 - val_loss: 2378.4021 - val_accuracy: 0.3667 - lr: 4.3980e-04\n",
            "Epoch 143/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 480680.0000 - accuracy: 0.5289 - val_loss: 177316.2188 - val_accuracy: 0.6333 - lr: 4.3980e-04\n",
            "Epoch 144/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 549290.7500 - accuracy: 0.5537 - val_loss: 628716.0625 - val_accuracy: 0.6333 - lr: 4.3980e-04\n",
            "Epoch 145/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 582336.0625 - accuracy: 0.5372 - val_loss: 29255.9961 - val_accuracy: 0.6333 - lr: 4.3980e-04\n",
            "Epoch 146/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 329545.0938 - accuracy: 0.5868 - val_loss: 274183.5312 - val_accuracy: 0.3667 - lr: 4.3980e-04\n",
            "Epoch 147/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 488313.1875 - accuracy: 0.4463 - val_loss: 46596.1172 - val_accuracy: 0.3667 - lr: 4.3980e-04\n",
            "Epoch 148/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 363174.3438 - accuracy: 0.5537 - val_loss: 165417.5312 - val_accuracy: 0.3667 - lr: 4.3980e-04\n",
            "Epoch 149/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 385761.0625 - accuracy: 0.5124 - val_loss: 593839.9375 - val_accuracy: 0.6333 - lr: 4.3980e-04\n",
            "Epoch 150/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 733437.9375 - accuracy: 0.5041 - val_loss: 189465.8594 - val_accuracy: 0.6333 - lr: 4.3980e-04\n",
            "Epoch 151/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 522045.8125 - accuracy: 0.4463 - val_loss: 437592.3750 - val_accuracy: 0.3667 - lr: 4.3980e-04\n",
            "Epoch 152/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 687490.5000 - accuracy: 0.5455 - val_loss: 661174.1875 - val_accuracy: 0.3667 - lr: 3.5184e-04\n",
            "Epoch 153/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 415855.1250 - accuracy: 0.5207 - val_loss: 21427.9980 - val_accuracy: 0.6333 - lr: 3.5184e-04\n",
            "Epoch 154/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 385009.4688 - accuracy: 0.5372 - val_loss: 310359.2812 - val_accuracy: 0.6333 - lr: 3.5184e-04\n",
            "Epoch 155/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 415613.7188 - accuracy: 0.5041 - val_loss: 121705.0547 - val_accuracy: 0.6333 - lr: 3.5184e-04\n",
            "Epoch 156/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 377841.4062 - accuracy: 0.4876 - val_loss: 22723.4570 - val_accuracy: 0.6333 - lr: 3.5184e-04\n",
            "Epoch 157/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 285975.6875 - accuracy: 0.5702 - val_loss: 196037.0938 - val_accuracy: 0.6333 - lr: 2.8147e-04\n",
            "Epoch 158/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 379582.0938 - accuracy: 0.5702 - val_loss: 705952.1250 - val_accuracy: 0.6333 - lr: 2.8147e-04\n",
            "Epoch 159/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 383448.5312 - accuracy: 0.5785 - val_loss: 371156.0312 - val_accuracy: 0.6333 - lr: 2.8147e-04\n",
            "Epoch 160/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 347790.1250 - accuracy: 0.5455 - val_loss: 259888.9844 - val_accuracy: 0.6333 - lr: 2.8147e-04\n",
            "Epoch 161/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 358823.1250 - accuracy: 0.4545 - val_loss: 288868.9688 - val_accuracy: 0.6333 - lr: 2.8147e-04\n",
            "Epoch 162/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 322731.4375 - accuracy: 0.5207 - val_loss: 161749.6875 - val_accuracy: 0.6333 - lr: 2.8147e-04\n",
            "Epoch 163/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 283111.9062 - accuracy: 0.5207 - val_loss: 96697.4844 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 164/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 243808.1562 - accuracy: 0.5785 - val_loss: 3400.2395 - val_accuracy: 0.3667 - lr: 2.2518e-04\n",
            "Epoch 165/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 269809.2812 - accuracy: 0.5207 - val_loss: 1571.1666 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 166/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 306309.0312 - accuracy: 0.4711 - val_loss: 316417.2188 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 167/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 299017.2812 - accuracy: 0.5785 - val_loss: 121054.1562 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 168/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 258555.7031 - accuracy: 0.5124 - val_loss: 57809.5977 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 169/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 216467.7969 - accuracy: 0.5289 - val_loss: 141367.6094 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 170/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 216499.5625 - accuracy: 0.5455 - val_loss: 257481.0156 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 171/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 276343.5625 - accuracy: 0.5124 - val_loss: 167839.3594 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 172/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 234523.3594 - accuracy: 0.5785 - val_loss: 25530.2637 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 173/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 264451.9062 - accuracy: 0.4711 - val_loss: 64277.9883 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 174/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 252756.0000 - accuracy: 0.5785 - val_loss: 275225.5938 - val_accuracy: 0.6333 - lr: 2.2518e-04\n",
            "Epoch 175/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 318065.8750 - accuracy: 0.4463 - val_loss: 232909.5000 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 176/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 292171.5000 - accuracy: 0.4711 - val_loss: 84992.1484 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 177/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 181851.0156 - accuracy: 0.5537 - val_loss: 107106.1484 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 178/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 180557.0312 - accuracy: 0.6198 - val_loss: 10066.7754 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 179/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 218361.1875 - accuracy: 0.5372 - val_loss: 38945.1758 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 180/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 165274.9688 - accuracy: 0.5620 - val_loss: 86921.3984 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 181/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 191866.0781 - accuracy: 0.5537 - val_loss: 24843.8496 - val_accuracy: 0.3667 - lr: 1.8014e-04\n",
            "Epoch 182/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 220015.0469 - accuracy: 0.5372 - val_loss: 1152.8895 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 183/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 242996.4688 - accuracy: 0.5537 - val_loss: 3170.2864 - val_accuracy: 0.3667 - lr: 1.8014e-04\n",
            "Epoch 184/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 166766.8906 - accuracy: 0.5702 - val_loss: 29823.2188 - val_accuracy: 0.6333 - lr: 1.8014e-04\n",
            "Epoch 185/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 211936.3750 - accuracy: 0.5537 - val_loss: 156258.9062 - val_accuracy: 0.3667 - lr: 1.8014e-04\n",
            "Epoch 186/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 255220.7500 - accuracy: 0.4545 - val_loss: 45183.6680 - val_accuracy: 0.3667 - lr: 1.4412e-04\n",
            "Epoch 187/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 191767.1094 - accuracy: 0.5702 - val_loss: 56834.7734 - val_accuracy: 0.6333 - lr: 1.4412e-04\n",
            "Epoch 188/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 228366.9688 - accuracy: 0.4463 - val_loss: 214206.1875 - val_accuracy: 0.6333 - lr: 1.4412e-04\n",
            "Epoch 189/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 173974.5469 - accuracy: 0.5702 - val_loss: 102814.8281 - val_accuracy: 0.6333 - lr: 1.4412e-04\n",
            "Epoch 190/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 186034.8594 - accuracy: 0.5289 - val_loss: 19657.6133 - val_accuracy: 0.6333 - lr: 1.4412e-04\n",
            "Epoch 191/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 214515.5625 - accuracy: 0.5041 - val_loss: 58112.4922 - val_accuracy: 0.3667 - lr: 1.1529e-04\n",
            "Epoch 192/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 163996.2812 - accuracy: 0.5868 - val_loss: 57723.6445 - val_accuracy: 0.3667 - lr: 1.1529e-04\n",
            "Epoch 193/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 126157.2891 - accuracy: 0.5950 - val_loss: 65098.8242 - val_accuracy: 0.6333 - lr: 1.1529e-04\n",
            "Epoch 194/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 159543.3438 - accuracy: 0.5289 - val_loss: 67318.6562 - val_accuracy: 0.6333 - lr: 1.1529e-04\n",
            "Epoch 195/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 166559.9062 - accuracy: 0.4793 - val_loss: 41482.5508 - val_accuracy: 0.6333 - lr: 1.1529e-04\n",
            "Epoch 196/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 134366.6719 - accuracy: 0.6116 - val_loss: 79931.7734 - val_accuracy: 0.6333 - lr: 1.1529e-04\n",
            "Epoch 197/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 135560.0781 - accuracy: 0.6116 - val_loss: 46318.5391 - val_accuracy: 0.6333 - lr: 1.1529e-04\n",
            "Epoch 198/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 165390.8594 - accuracy: 0.4876 - val_loss: 45171.7422 - val_accuracy: 0.6333 - lr: 1.1529e-04\n",
            "Epoch 199/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 152276.0469 - accuracy: 0.5620 - val_loss: 85642.2109 - val_accuracy: 0.6333 - lr: 9.2234e-05\n",
            "Epoch 200/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 162934.9219 - accuracy: 0.4711 - val_loss: 101230.6719 - val_accuracy: 0.6333 - lr: 9.2234e-05\n",
            "Epoch 201/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 126259.9609 - accuracy: 0.5537 - val_loss: 59722.9141 - val_accuracy: 0.6333 - lr: 9.2234e-05\n",
            "Epoch 202/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 131246.1562 - accuracy: 0.5537 - val_loss: 223480.1406 - val_accuracy: 0.3667 - lr: 9.2234e-05\n",
            "Epoch 203/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 152859.9688 - accuracy: 0.5207 - val_loss: 104441.2656 - val_accuracy: 0.6333 - lr: 9.2234e-05\n",
            "Epoch 204/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 144545.7969 - accuracy: 0.5289 - val_loss: 59110.8320 - val_accuracy: 0.6333 - lr: 7.3787e-05\n",
            "Epoch 205/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 117441.3828 - accuracy: 0.5207 - val_loss: 54353.1211 - val_accuracy: 0.3667 - lr: 7.3787e-05\n",
            "Epoch 206/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 135128.2500 - accuracy: 0.5950 - val_loss: 52467.4922 - val_accuracy: 0.6333 - lr: 7.3787e-05\n",
            "Epoch 207/300\n",
            "31/31 [==============================] - 2s 64ms/step - loss: 188042.8750 - accuracy: 0.4545 - val_loss: 26612.2793 - val_accuracy: 0.3667 - lr: 7.3787e-05\n",
            "Epoch 208/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 128254.8125 - accuracy: 0.4876 - val_loss: 53019.0547 - val_accuracy: 0.6333 - lr: 7.3787e-05\n",
            "Epoch 209/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 131276.6875 - accuracy: 0.5207 - val_loss: 9511.0459 - val_accuracy: 0.3667 - lr: 7.3787e-05\n",
            "Epoch 210/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 123797.2031 - accuracy: 0.5207 - val_loss: 46428.0898 - val_accuracy: 0.6333 - lr: 7.3787e-05\n",
            "Epoch 211/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 121563.4062 - accuracy: 0.5620 - val_loss: 45245.3320 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 212/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 136123.5312 - accuracy: 0.5372 - val_loss: 19444.0215 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 213/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 111168.5078 - accuracy: 0.5455 - val_loss: 3180.7000 - val_accuracy: 0.3667 - lr: 5.9030e-05\n",
            "Epoch 214/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 133607.9375 - accuracy: 0.4876 - val_loss: 114758.5000 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 215/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 158035.1406 - accuracy: 0.4545 - val_loss: 29840.3828 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 216/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 125676.6719 - accuracy: 0.5868 - val_loss: 4590.8188 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 217/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 113074.5625 - accuracy: 0.5620 - val_loss: 42379.0234 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 218/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 104961.7969 - accuracy: 0.5289 - val_loss: 32703.6172 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 219/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 106544.2969 - accuracy: 0.5455 - val_loss: 41429.5000 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 220/300\n",
            "31/31 [==============================] - 2s 63ms/step - loss: 127645.1641 - accuracy: 0.4959 - val_loss: 16490.0625 - val_accuracy: 0.3667 - lr: 5.9030e-05\n",
            "Epoch 221/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 113944.8281 - accuracy: 0.5455 - val_loss: 11703.1777 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 222/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 139055.7656 - accuracy: 0.4545 - val_loss: 50784.9844 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 223/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 133541.2969 - accuracy: 0.4298 - val_loss: 81772.0859 - val_accuracy: 0.6333 - lr: 5.9030e-05\n",
            "Epoch 224/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 106726.1719 - accuracy: 0.5124 - val_loss: 36456.7500 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 225/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 104344.0312 - accuracy: 0.5207 - val_loss: 29370.6289 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 226/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 124632.3359 - accuracy: 0.4959 - val_loss: 54790.7070 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 227/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 117932.8359 - accuracy: 0.5950 - val_loss: 2505.1636 - val_accuracy: 0.3667 - lr: 4.7224e-05\n",
            "Epoch 228/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 92895.1875 - accuracy: 0.5537 - val_loss: 48416.9570 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 229/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 106369.7656 - accuracy: 0.5207 - val_loss: 7704.3862 - val_accuracy: 0.3667 - lr: 4.7224e-05\n",
            "Epoch 230/300\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 114580.0625 - accuracy: 0.5041 - val_loss: 41184.9766 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 231/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 105606.6953 - accuracy: 0.4793 - val_loss: 64515.5742 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 232/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 86307.6016 - accuracy: 0.5702 - val_loss: 60762.2266 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 233/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 76598.6953 - accuracy: 0.6116 - val_loss: 35859.2969 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 234/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 95274.5234 - accuracy: 0.5455 - val_loss: 41931.0273 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 235/300\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 113091.6016 - accuracy: 0.4463 - val_loss: 599.5750 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 236/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 102641.4609 - accuracy: 0.5041 - val_loss: 18316.4336 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 237/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 98705.0391 - accuracy: 0.5620 - val_loss: 11911.3477 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 238/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 127219.2578 - accuracy: 0.4793 - val_loss: 10866.5723 - val_accuracy: 0.6333 - lr: 4.7224e-05\n",
            "Epoch 239/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 116574.3828 - accuracy: 0.4628 - val_loss: 39139.4531 - val_accuracy: 0.6333 - lr: 3.7779e-05\n",
            "Epoch 240/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 79480.7109 - accuracy: 0.6033 - val_loss: 17759.1582 - val_accuracy: 0.6333 - lr: 3.7779e-05\n",
            "Epoch 241/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 126370.9453 - accuracy: 0.4876 - val_loss: 7443.5845 - val_accuracy: 0.6333 - lr: 3.7779e-05\n",
            "Epoch 242/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 108879.3516 - accuracy: 0.5537 - val_loss: 35409.0000 - val_accuracy: 0.3667 - lr: 3.7779e-05\n",
            "Epoch 243/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 124177.4141 - accuracy: 0.4215 - val_loss: 52589.2734 - val_accuracy: 0.6333 - lr: 3.7779e-05\n",
            "Epoch 244/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 93963.1016 - accuracy: 0.4463 - val_loss: 27841.3203 - val_accuracy: 0.6333 - lr: 3.0223e-05\n",
            "Epoch 245/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 82099.2500 - accuracy: 0.5207 - val_loss: 21414.9297 - val_accuracy: 0.6333 - lr: 3.0223e-05\n",
            "Epoch 246/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 89591.1484 - accuracy: 0.5289 - val_loss: 46887.2891 - val_accuracy: 0.6333 - lr: 3.0223e-05\n",
            "Epoch 247/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 94465.3906 - accuracy: 0.4711 - val_loss: 34441.3320 - val_accuracy: 0.6333 - lr: 3.0223e-05\n",
            "Epoch 248/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 105400.2344 - accuracy: 0.5289 - val_loss: 47639.3867 - val_accuracy: 0.6333 - lr: 3.0223e-05\n",
            "Epoch 249/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 101027.5547 - accuracy: 0.5537 - val_loss: 28678.1250 - val_accuracy: 0.6333 - lr: 2.4179e-05\n",
            "Epoch 250/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 111560.6094 - accuracy: 0.4711 - val_loss: 20761.7422 - val_accuracy: 0.6333 - lr: 2.4179e-05\n",
            "Epoch 251/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 86752.7422 - accuracy: 0.4959 - val_loss: 43236.7422 - val_accuracy: 0.6333 - lr: 2.4179e-05\n",
            "Epoch 252/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 99262.4219 - accuracy: 0.4711 - val_loss: 33302.6406 - val_accuracy: 0.6333 - lr: 2.4179e-05\n",
            "Epoch 253/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 80999.4531 - accuracy: 0.5372 - val_loss: 31727.7520 - val_accuracy: 0.6333 - lr: 2.4179e-05\n",
            "Epoch 254/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 85478.3125 - accuracy: 0.5372 - val_loss: 24123.4121 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 255/300\n",
            "31/31 [==============================] - 2s 68ms/step - loss: 76964.5547 - accuracy: 0.5124 - val_loss: 33609.2422 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 256/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 72595.9766 - accuracy: 0.5537 - val_loss: 13488.8848 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 257/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 83365.7344 - accuracy: 0.5702 - val_loss: 19599.8379 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 258/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 70233.8750 - accuracy: 0.5537 - val_loss: 52732.3164 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 259/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 102010.9141 - accuracy: 0.5041 - val_loss: 17825.1836 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 260/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 82677.5234 - accuracy: 0.5702 - val_loss: 34612.3125 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 261/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 81895.9531 - accuracy: 0.5289 - val_loss: 15501.4336 - val_accuracy: 0.3667 - lr: 1.9343e-05\n",
            "Epoch 262/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 64928.7891 - accuracy: 0.5620 - val_loss: 41586.3516 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 263/300\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 81921.0078 - accuracy: 0.5372 - val_loss: 34189.5195 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 264/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 86222.6484 - accuracy: 0.4793 - val_loss: 10848.4531 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 265/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 90933.7344 - accuracy: 0.4959 - val_loss: 46372.7344 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 266/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 75041.4688 - accuracy: 0.5041 - val_loss: 33166.9453 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 267/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 80579.2500 - accuracy: 0.5537 - val_loss: 41897.0508 - val_accuracy: 0.6333 - lr: 1.9343e-05\n",
            "Epoch 268/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 53854.8008 - accuracy: 0.6116 - val_loss: 4808.9741 - val_accuracy: 0.6333 - lr: 1.5474e-05\n",
            "Epoch 269/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 87064.8750 - accuracy: 0.4876 - val_loss: 39522.3125 - val_accuracy: 0.6333 - lr: 1.5474e-05\n",
            "Epoch 270/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 67398.8438 - accuracy: 0.5785 - val_loss: 13081.6064 - val_accuracy: 0.6333 - lr: 1.5474e-05\n",
            "Epoch 271/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 78155.3984 - accuracy: 0.5289 - val_loss: 34407.6367 - val_accuracy: 0.6333 - lr: 1.5474e-05\n",
            "Epoch 272/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 82157.1484 - accuracy: 0.5207 - val_loss: 43215.3047 - val_accuracy: 0.6333 - lr: 1.5474e-05\n",
            "Epoch 273/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 64181.8711 - accuracy: 0.5702 - val_loss: 6113.2739 - val_accuracy: 0.3667 - lr: 1.5474e-05\n",
            "Epoch 274/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 94174.0234 - accuracy: 0.5289 - val_loss: 24203.2617 - val_accuracy: 0.6333 - lr: 1.2379e-05\n",
            "Epoch 275/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 89119.0547 - accuracy: 0.4711 - val_loss: 17270.7285 - val_accuracy: 0.6333 - lr: 1.2379e-05\n",
            "Epoch 276/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 68647.2422 - accuracy: 0.5950 - val_loss: 26425.5039 - val_accuracy: 0.6333 - lr: 1.2379e-05\n",
            "Epoch 277/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 76544.5312 - accuracy: 0.5289 - val_loss: 14561.4932 - val_accuracy: 0.6333 - lr: 1.2379e-05\n",
            "Epoch 278/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 77664.4844 - accuracy: 0.4711 - val_loss: 17499.7461 - val_accuracy: 0.6333 - lr: 1.2379e-05\n",
            "Epoch 279/300\n",
            "31/31 [==============================] - 2s 65ms/step - loss: 80684.5859 - accuracy: 0.5455 - val_loss: 33280.7422 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 280/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 76300.5547 - accuracy: 0.5207 - val_loss: 19847.2500 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 281/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 78886.9688 - accuracy: 0.4959 - val_loss: 23699.0977 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 282/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 90905.6016 - accuracy: 0.5124 - val_loss: 33390.2383 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 283/300\n",
            "31/31 [==============================] - 2s 61ms/step - loss: 85094.0625 - accuracy: 0.5537 - val_loss: 5857.7646 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 284/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 68423.3516 - accuracy: 0.5124 - val_loss: 25435.6445 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 285/300\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 65309.9375 - accuracy: 0.5950 - val_loss: 21108.7715 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 286/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 72832.3750 - accuracy: 0.5289 - val_loss: 24859.4453 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 287/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 64089.5664 - accuracy: 0.5868 - val_loss: 29065.3477 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 288/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 76919.5859 - accuracy: 0.4959 - val_loss: 19194.5664 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 289/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 75285.6172 - accuracy: 0.5041 - val_loss: 21059.6484 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 290/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 64105.1719 - accuracy: 0.5372 - val_loss: 27723.6758 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 291/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 59878.1289 - accuracy: 0.6033 - val_loss: 6864.4697 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 292/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 78702.0156 - accuracy: 0.4628 - val_loss: 26650.7852 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 293/300\n",
            "31/31 [==============================] - 2s 60ms/step - loss: 77603.0547 - accuracy: 0.5372 - val_loss: 19924.7324 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 294/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 72218.7266 - accuracy: 0.5537 - val_loss: 22000.1172 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 295/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 81068.8516 - accuracy: 0.4793 - val_loss: 14794.8340 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 296/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 64986.6758 - accuracy: 0.5289 - val_loss: 26535.4883 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 297/300\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 71048.1328 - accuracy: 0.5785 - val_loss: 28541.9590 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 298/300\n",
            "31/31 [==============================] - 2s 62ms/step - loss: 58414.8828 - accuracy: 0.5868 - val_loss: 30254.6328 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 299/300\n",
            "31/31 [==============================] - 2s 58ms/step - loss: 86202.7734 - accuracy: 0.5455 - val_loss: 26903.3672 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 300/300\n",
            "31/31 [==============================] - 2s 59ms/step - loss: 80692.9922 - accuracy: 0.5207 - val_loss: 28706.3184 - val_accuracy: 0.6333 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "encoder.trainable=False\n",
        "\n",
        "casual   = Input(shape=(IMG_SIZE, IMG_SIZE, 3), name=\"casual\")\n",
        "\n",
        "encoded_casual = encoder(casual)\n",
        "x = Flatten()(encoded_casual)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(4096)(x)\n",
        "x = Dropout(0.3)(x)\n",
        "out = Dense(NUM_CLASSES, activation=\"softmax\")(x)\n",
        "\n",
        "classifier = Model(inputs = casual, outputs = out, name='fuc_IVF_with_class')\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-2)\n",
        "loss = 'categorical_crossentropy'\n",
        "#loss = tf.keras.losses.CategoricalHinge()\n",
        "metrics = ['accuracy']\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=loss, metrics=metrics#[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./model_checkpoint/modelv6.1',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8,patience=5, min_lr=0.00001),\n",
        "    model_checkpoint_callback,\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]\n",
        "\n",
        "epochs = 300  # @param {type: \"slider\", min:1, max:1000}\n",
        "hist = classifier.fit(casual_generator,\n",
        "                 validation_data = validation_generator,\n",
        "                 epochs=epochs,\n",
        "                 #steps_per_epoch = 150//4,\n",
        "                 callbacks=my_callbacks,\n",
        "                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITzRULLqMFtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6877058-7080-4b5b-98d7-11ef02b809d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "31/31 [==============================] - 18s 270ms/step - loss: 2356.6985 - accuracy: 0.5950 - val_loss: 1.3947 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 2/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 5.9686 - accuracy: 0.5455 - val_loss: 1.2690 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 3/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 1.9170 - accuracy: 0.4628 - val_loss: 0.6909 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 4/300\n",
            "31/31 [==============================] - 4s 130ms/step - loss: 1.1773 - accuracy: 0.5702 - val_loss: 0.7110 - val_accuracy: 0.5333 - lr: 1.0000e-04\n",
            "Epoch 5/300\n",
            "31/31 [==============================] - 4s 130ms/step - loss: 0.8127 - accuracy: 0.5537 - val_loss: 0.6667 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 6/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.8795 - accuracy: 0.4959 - val_loss: 0.6555 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 7/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.7958 - accuracy: 0.4959 - val_loss: 0.6758 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 8/300\n",
            "31/31 [==============================] - 7s 238ms/step - loss: 0.8590 - accuracy: 0.4793 - val_loss: 0.6581 - val_accuracy: 0.7000 - lr: 1.0000e-04\n",
            "Epoch 9/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.9159 - accuracy: 0.4876 - val_loss: 0.9995 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 10/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.8545 - accuracy: 0.5620 - val_loss: 0.6894 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 11/300\n",
            "31/31 [==============================] - 4s 131ms/step - loss: 0.7532 - accuracy: 0.5372 - val_loss: 0.6661 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 12/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.7218 - accuracy: 0.5868 - val_loss: 0.6545 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 13/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.7708 - accuracy: 0.5289 - val_loss: 0.6677 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 14/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.7525 - accuracy: 0.5372 - val_loss: 0.6945 - val_accuracy: 0.5333 - lr: 1.0000e-04\n",
            "Epoch 15/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7189 - accuracy: 0.5124 - val_loss: 0.6911 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 16/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7484 - accuracy: 0.5372 - val_loss: 0.6513 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 17/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 1.7013 - accuracy: 0.5372 - val_loss: 0.9709 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 18/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.9765 - accuracy: 0.5702 - val_loss: 0.6207 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 19/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.7990 - accuracy: 0.5620 - val_loss: 0.6636 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 20/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.8054 - accuracy: 0.5124 - val_loss: 0.6922 - val_accuracy: 0.6333 - lr: 1.0000e-04\n",
            "Epoch 21/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.8667 - accuracy: 0.5620 - val_loss: 0.6655 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 22/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.8111 - accuracy: 0.5207 - val_loss: 0.6690 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 23/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.8326 - accuracy: 0.5455 - val_loss: 0.6760 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 24/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.7750 - accuracy: 0.5537 - val_loss: 0.6459 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 25/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6503 - accuracy: 0.5950 - val_loss: 0.8218 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 26/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 1.1500 - accuracy: 0.6116 - val_loss: 0.6509 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 27/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7687 - accuracy: 0.5537 - val_loss: 0.6558 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 28/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6632 - accuracy: 0.6446 - val_loss: 0.7922 - val_accuracy: 0.3667 - lr: 8.0000e-05\n",
            "Epoch 29/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7076 - accuracy: 0.5455 - val_loss: 0.7095 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 30/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7611 - accuracy: 0.5455 - val_loss: 0.6683 - val_accuracy: 0.6333 - lr: 8.0000e-05\n",
            "Epoch 31/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6925 - accuracy: 0.5372 - val_loss: 0.6723 - val_accuracy: 0.6333 - lr: 6.4000e-05\n",
            "Epoch 32/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7198 - accuracy: 0.5455 - val_loss: 0.6504 - val_accuracy: 0.6333 - lr: 6.4000e-05\n",
            "Epoch 33/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.7363 - accuracy: 0.5785 - val_loss: 0.6537 - val_accuracy: 0.6333 - lr: 6.4000e-05\n",
            "Epoch 34/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7158 - accuracy: 0.5455 - val_loss: 0.6583 - val_accuracy: 0.6333 - lr: 6.4000e-05\n",
            "Epoch 35/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6942 - accuracy: 0.5868 - val_loss: 0.6551 - val_accuracy: 0.6333 - lr: 6.4000e-05\n",
            "Epoch 36/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6870 - accuracy: 0.6198 - val_loss: 0.6631 - val_accuracy: 0.6333 - lr: 5.1200e-05\n",
            "Epoch 37/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6754 - accuracy: 0.6281 - val_loss: 0.6565 - val_accuracy: 0.6333 - lr: 5.1200e-05\n",
            "Epoch 38/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6941 - accuracy: 0.5950 - val_loss: 0.6596 - val_accuracy: 0.6333 - lr: 5.1200e-05\n",
            "Epoch 39/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7136 - accuracy: 0.5950 - val_loss: 0.6736 - val_accuracy: 0.5667 - lr: 5.1200e-05\n",
            "Epoch 40/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.8761 - accuracy: 0.5620 - val_loss: 0.6747 - val_accuracy: 0.6333 - lr: 5.1200e-05\n",
            "Epoch 41/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7741 - accuracy: 0.5950 - val_loss: 0.6584 - val_accuracy: 0.6333 - lr: 4.0960e-05\n",
            "Epoch 42/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7264 - accuracy: 0.5455 - val_loss: 0.6598 - val_accuracy: 0.6333 - lr: 4.0960e-05\n",
            "Epoch 43/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6889 - accuracy: 0.5372 - val_loss: 0.6579 - val_accuracy: 0.6333 - lr: 4.0960e-05\n",
            "Epoch 44/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6726 - accuracy: 0.6198 - val_loss: 0.6553 - val_accuracy: 0.6333 - lr: 4.0960e-05\n",
            "Epoch 45/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6849 - accuracy: 0.6116 - val_loss: 0.6781 - val_accuracy: 0.6333 - lr: 4.0960e-05\n",
            "Epoch 46/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6843 - accuracy: 0.5868 - val_loss: 0.6563 - val_accuracy: 0.6333 - lr: 3.2768e-05\n",
            "Epoch 47/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.7051 - accuracy: 0.5702 - val_loss: 0.6617 - val_accuracy: 0.6333 - lr: 3.2768e-05\n",
            "Epoch 48/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6668 - accuracy: 0.6116 - val_loss: 0.6673 - val_accuracy: 0.6333 - lr: 3.2768e-05\n",
            "Epoch 49/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6788 - accuracy: 0.6033 - val_loss: 0.6591 - val_accuracy: 0.6333 - lr: 3.2768e-05\n",
            "Epoch 50/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6753 - accuracy: 0.5372 - val_loss: 0.6579 - val_accuracy: 0.6333 - lr: 3.2768e-05\n",
            "Epoch 51/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6546 - accuracy: 0.6281 - val_loss: 0.6589 - val_accuracy: 0.6333 - lr: 2.6214e-05\n",
            "Epoch 52/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6647 - accuracy: 0.6116 - val_loss: 0.6570 - val_accuracy: 0.6333 - lr: 2.6214e-05\n",
            "Epoch 53/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6654 - accuracy: 0.6364 - val_loss: 0.6567 - val_accuracy: 0.6333 - lr: 2.6214e-05\n",
            "Epoch 54/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6849 - accuracy: 0.5537 - val_loss: 0.6551 - val_accuracy: 0.6333 - lr: 2.6214e-05\n",
            "Epoch 55/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6683 - accuracy: 0.5868 - val_loss: 0.6564 - val_accuracy: 0.6333 - lr: 2.6214e-05\n",
            "Epoch 56/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6985 - accuracy: 0.6281 - val_loss: 0.6540 - val_accuracy: 0.6333 - lr: 2.0972e-05\n",
            "Epoch 57/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6824 - accuracy: 0.6116 - val_loss: 0.6549 - val_accuracy: 0.6333 - lr: 2.0972e-05\n",
            "Epoch 58/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6550 - accuracy: 0.6364 - val_loss: 0.6601 - val_accuracy: 0.6333 - lr: 2.0972e-05\n",
            "Epoch 59/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.7040 - accuracy: 0.5785 - val_loss: 0.6556 - val_accuracy: 0.6333 - lr: 2.0972e-05\n",
            "Epoch 60/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6676 - accuracy: 0.6198 - val_loss: 0.6576 - val_accuracy: 0.6333 - lr: 2.0972e-05\n",
            "Epoch 61/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6867 - accuracy: 0.6281 - val_loss: 0.6584 - val_accuracy: 0.6333 - lr: 1.6777e-05\n",
            "Epoch 62/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6756 - accuracy: 0.6281 - val_loss: 0.6570 - val_accuracy: 0.6333 - lr: 1.6777e-05\n",
            "Epoch 63/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6630 - accuracy: 0.6198 - val_loss: 0.6550 - val_accuracy: 0.6333 - lr: 1.6777e-05\n",
            "Epoch 64/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6855 - accuracy: 0.6033 - val_loss: 0.6574 - val_accuracy: 0.6333 - lr: 1.6777e-05\n",
            "Epoch 65/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6890 - accuracy: 0.6116 - val_loss: 0.6564 - val_accuracy: 0.6333 - lr: 1.6777e-05\n",
            "Epoch 66/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6511 - accuracy: 0.6281 - val_loss: 0.6570 - val_accuracy: 0.6333 - lr: 1.3422e-05\n",
            "Epoch 67/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6679 - accuracy: 0.6446 - val_loss: 0.6563 - val_accuracy: 0.6333 - lr: 1.3422e-05\n",
            "Epoch 68/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6609 - accuracy: 0.6281 - val_loss: 0.6553 - val_accuracy: 0.6333 - lr: 1.3422e-05\n",
            "Epoch 69/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6621 - accuracy: 0.6281 - val_loss: 0.6558 - val_accuracy: 0.6333 - lr: 1.3422e-05\n",
            "Epoch 70/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6609 - accuracy: 0.6446 - val_loss: 0.6557 - val_accuracy: 0.6333 - lr: 1.3422e-05\n",
            "Epoch 71/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6593 - accuracy: 0.6446 - val_loss: 0.6579 - val_accuracy: 0.6333 - lr: 1.0737e-05\n",
            "Epoch 72/300\n",
            "31/31 [==============================] - 4s 124ms/step - loss: 0.6742 - accuracy: 0.6116 - val_loss: 0.6567 - val_accuracy: 0.6333 - lr: 1.0737e-05\n",
            "Epoch 73/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6826 - accuracy: 0.6281 - val_loss: 0.6567 - val_accuracy: 0.6333 - lr: 1.0737e-05\n",
            "Epoch 74/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6788 - accuracy: 0.6364 - val_loss: 0.6566 - val_accuracy: 0.6333 - lr: 1.0737e-05\n",
            "Epoch 75/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6671 - accuracy: 0.6364 - val_loss: 0.6552 - val_accuracy: 0.6333 - lr: 1.0737e-05\n",
            "Epoch 76/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6797 - accuracy: 0.6116 - val_loss: 0.6574 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 77/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6722 - accuracy: 0.6198 - val_loss: 0.6575 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 78/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6611 - accuracy: 0.6446 - val_loss: 0.6554 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 79/300\n",
            "31/31 [==============================] - 4s 125ms/step - loss: 0.6652 - accuracy: 0.6198 - val_loss: 0.6592 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 80/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6652 - accuracy: 0.6446 - val_loss: 0.6572 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 81/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6626 - accuracy: 0.6281 - val_loss: 0.6570 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 82/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6455 - accuracy: 0.6281 - val_loss: 0.6559 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 83/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6678 - accuracy: 0.6446 - val_loss: 0.6555 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 84/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6674 - accuracy: 0.6033 - val_loss: 0.6571 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 85/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6674 - accuracy: 0.6198 - val_loss: 0.6556 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 86/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6799 - accuracy: 0.6116 - val_loss: 0.6564 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 87/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6751 - accuracy: 0.6198 - val_loss: 0.6563 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 88/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6628 - accuracy: 0.6281 - val_loss: 0.6566 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 89/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6717 - accuracy: 0.6281 - val_loss: 0.6566 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 90/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6597 - accuracy: 0.6364 - val_loss: 0.6575 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 91/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6587 - accuracy: 0.6364 - val_loss: 0.6560 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 92/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6446 - accuracy: 0.6281 - val_loss: 0.6569 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 93/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6876 - accuracy: 0.6198 - val_loss: 0.6562 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 94/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6402 - accuracy: 0.6281 - val_loss: 0.6572 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 95/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6870 - accuracy: 0.6198 - val_loss: 0.6583 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 96/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6771 - accuracy: 0.6198 - val_loss: 0.6583 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 97/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6550 - accuracy: 0.6116 - val_loss: 0.6580 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 98/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6698 - accuracy: 0.6198 - val_loss: 0.6580 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 99/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6752 - accuracy: 0.6281 - val_loss: 0.6566 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 100/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6891 - accuracy: 0.6198 - val_loss: 0.6564 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 101/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6700 - accuracy: 0.6198 - val_loss: 0.6574 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 102/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6656 - accuracy: 0.6529 - val_loss: 0.6565 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 103/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6713 - accuracy: 0.6198 - val_loss: 0.6563 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 104/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6559 - accuracy: 0.6116 - val_loss: 0.6544 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 105/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6599 - accuracy: 0.6116 - val_loss: 0.6573 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 106/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6966 - accuracy: 0.6116 - val_loss: 0.6570 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 107/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6610 - accuracy: 0.6281 - val_loss: 0.6571 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 108/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6586 - accuracy: 0.6281 - val_loss: 0.6575 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 109/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6613 - accuracy: 0.6364 - val_loss: 0.6572 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 110/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6523 - accuracy: 0.6033 - val_loss: 0.6586 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 111/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6531 - accuracy: 0.6116 - val_loss: 0.6566 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 112/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6585 - accuracy: 0.6116 - val_loss: 0.6551 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 113/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6498 - accuracy: 0.6281 - val_loss: 0.6577 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 114/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6779 - accuracy: 0.6281 - val_loss: 0.6568 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 115/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6698 - accuracy: 0.6198 - val_loss: 0.6563 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 116/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6637 - accuracy: 0.6281 - val_loss: 0.6564 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 117/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6724 - accuracy: 0.6281 - val_loss: 0.6549 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 118/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6688 - accuracy: 0.6364 - val_loss: 0.6584 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 119/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6444 - accuracy: 0.6116 - val_loss: 0.6553 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 120/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6861 - accuracy: 0.5868 - val_loss: 0.6574 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 121/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6556 - accuracy: 0.6281 - val_loss: 0.6573 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 122/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6760 - accuracy: 0.6281 - val_loss: 0.6577 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 123/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6773 - accuracy: 0.5950 - val_loss: 0.6583 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 124/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6696 - accuracy: 0.6116 - val_loss: 0.6582 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 125/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6678 - accuracy: 0.6281 - val_loss: 0.6583 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 126/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6560 - accuracy: 0.6198 - val_loss: 0.6607 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 127/300\n",
            "31/31 [==============================] - 4s 130ms/step - loss: 0.6701 - accuracy: 0.6446 - val_loss: 0.6555 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 128/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6570 - accuracy: 0.6198 - val_loss: 0.6540 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 129/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6679 - accuracy: 0.6033 - val_loss: 0.6526 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 130/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6938 - accuracy: 0.5620 - val_loss: 0.6526 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 131/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6739 - accuracy: 0.6198 - val_loss: 0.6595 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 132/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6801 - accuracy: 0.6364 - val_loss: 0.6551 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 133/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6447 - accuracy: 0.6364 - val_loss: 0.6544 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 134/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6660 - accuracy: 0.6281 - val_loss: 0.6535 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 135/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6576 - accuracy: 0.6281 - val_loss: 0.6523 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 136/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6550 - accuracy: 0.6281 - val_loss: 0.6495 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 137/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6725 - accuracy: 0.6281 - val_loss: 0.6545 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 138/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6636 - accuracy: 0.6198 - val_loss: 0.6518 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 139/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6924 - accuracy: 0.6281 - val_loss: 0.6574 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 140/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6813 - accuracy: 0.6281 - val_loss: 0.6510 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 141/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6595 - accuracy: 0.6198 - val_loss: 0.6573 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 142/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6538 - accuracy: 0.6198 - val_loss: 0.6540 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 143/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6643 - accuracy: 0.6033 - val_loss: 0.6532 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 144/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6887 - accuracy: 0.6033 - val_loss: 0.6540 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 145/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6633 - accuracy: 0.6116 - val_loss: 0.6486 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 146/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6600 - accuracy: 0.6281 - val_loss: 0.6500 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 147/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6704 - accuracy: 0.6364 - val_loss: 0.6486 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 148/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6647 - accuracy: 0.6198 - val_loss: 0.6498 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 149/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6680 - accuracy: 0.6116 - val_loss: 0.6614 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 150/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6531 - accuracy: 0.6364 - val_loss: 0.6523 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 151/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6612 - accuracy: 0.6446 - val_loss: 0.6517 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 152/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6472 - accuracy: 0.6446 - val_loss: 0.6513 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 153/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6450 - accuracy: 0.6281 - val_loss: 0.6493 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 154/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6576 - accuracy: 0.6116 - val_loss: 0.6515 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 155/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6577 - accuracy: 0.6281 - val_loss: 0.6496 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 156/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6612 - accuracy: 0.6364 - val_loss: 0.6440 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 157/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6538 - accuracy: 0.6364 - val_loss: 0.6445 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 158/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6497 - accuracy: 0.6281 - val_loss: 0.6441 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 159/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6708 - accuracy: 0.6281 - val_loss: 0.6496 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 160/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6726 - accuracy: 0.6281 - val_loss: 0.6514 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 161/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6684 - accuracy: 0.6364 - val_loss: 0.6487 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 162/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6267 - accuracy: 0.6281 - val_loss: 0.6413 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 163/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6776 - accuracy: 0.6033 - val_loss: 0.6621 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 164/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6659 - accuracy: 0.6281 - val_loss: 0.6631 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 165/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6727 - accuracy: 0.6198 - val_loss: 0.6552 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 166/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6696 - accuracy: 0.6446 - val_loss: 0.6575 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 167/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6769 - accuracy: 0.6364 - val_loss: 0.6542 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 168/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6727 - accuracy: 0.6281 - val_loss: 0.6486 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 169/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6783 - accuracy: 0.6116 - val_loss: 0.6521 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 170/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6797 - accuracy: 0.6364 - val_loss: 0.6542 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 171/300\n",
            "31/31 [==============================] - 4s 129ms/step - loss: 0.6483 - accuracy: 0.6364 - val_loss: 0.6441 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 172/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6538 - accuracy: 0.6364 - val_loss: 0.6411 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 173/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6786 - accuracy: 0.6033 - val_loss: 0.6452 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 174/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6740 - accuracy: 0.6116 - val_loss: 0.6459 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 175/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6537 - accuracy: 0.6364 - val_loss: 0.6474 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 176/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6582 - accuracy: 0.6529 - val_loss: 0.6445 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 177/300\n",
            "31/31 [==============================] - 4s 129ms/step - loss: 0.6461 - accuracy: 0.6529 - val_loss: 0.6537 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 178/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6604 - accuracy: 0.6198 - val_loss: 0.6522 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 179/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6811 - accuracy: 0.5868 - val_loss: 0.6465 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 180/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6783 - accuracy: 0.6364 - val_loss: 0.6495 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 181/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6402 - accuracy: 0.6198 - val_loss: 0.6497 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 182/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6261 - accuracy: 0.6777 - val_loss: 0.6628 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 183/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6762 - accuracy: 0.6281 - val_loss: 0.6346 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 184/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6755 - accuracy: 0.6694 - val_loss: 0.6629 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 185/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6881 - accuracy: 0.6198 - val_loss: 0.6501 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 186/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.7019 - accuracy: 0.5702 - val_loss: 0.6554 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 187/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6414 - accuracy: 0.6694 - val_loss: 0.6510 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 188/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6706 - accuracy: 0.6281 - val_loss: 0.6544 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 189/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6563 - accuracy: 0.6364 - val_loss: 0.6570 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 190/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6896 - accuracy: 0.6116 - val_loss: 0.6458 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 191/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6549 - accuracy: 0.6198 - val_loss: 0.6571 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 192/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6816 - accuracy: 0.6198 - val_loss: 0.6476 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 193/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6691 - accuracy: 0.6529 - val_loss: 0.6472 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 194/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6282 - accuracy: 0.6612 - val_loss: 0.6466 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 195/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6591 - accuracy: 0.6198 - val_loss: 0.6474 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 196/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6748 - accuracy: 0.6281 - val_loss: 0.6691 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 197/300\n",
            "31/31 [==============================] - 4s 129ms/step - loss: 0.6625 - accuracy: 0.6033 - val_loss: 0.6498 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 198/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6662 - accuracy: 0.6033 - val_loss: 0.6476 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 199/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6657 - accuracy: 0.6198 - val_loss: 0.6655 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 200/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6771 - accuracy: 0.6033 - val_loss: 0.6623 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 201/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6578 - accuracy: 0.6116 - val_loss: 0.6536 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 202/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6637 - accuracy: 0.6033 - val_loss: 0.6588 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 203/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6951 - accuracy: 0.5950 - val_loss: 0.6344 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 204/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6653 - accuracy: 0.6281 - val_loss: 0.6427 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 205/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6783 - accuracy: 0.6529 - val_loss: 0.6427 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 206/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6588 - accuracy: 0.6198 - val_loss: 0.6610 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 207/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6731 - accuracy: 0.6198 - val_loss: 0.6465 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 208/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6688 - accuracy: 0.5950 - val_loss: 0.6433 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 209/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6524 - accuracy: 0.6364 - val_loss: 0.6485 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 210/300\n",
            "31/31 [==============================] - 6s 194ms/step - loss: 0.6370 - accuracy: 0.6860 - val_loss: 0.6540 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 211/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7005 - accuracy: 0.6281 - val_loss: 0.6344 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 212/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6708 - accuracy: 0.6694 - val_loss: 0.6485 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 213/300\n",
            "31/31 [==============================] - 4s 130ms/step - loss: 0.6462 - accuracy: 0.6529 - val_loss: 0.6363 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 214/300\n",
            "31/31 [==============================] - 4s 132ms/step - loss: 0.6640 - accuracy: 0.5702 - val_loss: 0.6390 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 215/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6808 - accuracy: 0.6033 - val_loss: 0.6348 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 216/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6559 - accuracy: 0.6198 - val_loss: 0.6390 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 217/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6541 - accuracy: 0.6446 - val_loss: 0.6735 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 218/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6532 - accuracy: 0.6446 - val_loss: 0.6297 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 219/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6814 - accuracy: 0.6116 - val_loss: 0.6428 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 220/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6603 - accuracy: 0.6198 - val_loss: 0.6210 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 221/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6317 - accuracy: 0.7107 - val_loss: 0.6518 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 222/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6635 - accuracy: 0.6529 - val_loss: 0.6502 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 223/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6578 - accuracy: 0.6364 - val_loss: 0.6432 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 224/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6389 - accuracy: 0.6446 - val_loss: 0.6390 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 225/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6799 - accuracy: 0.6198 - val_loss: 0.6606 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 226/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6930 - accuracy: 0.6033 - val_loss: 0.6531 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 227/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6784 - accuracy: 0.6364 - val_loss: 0.6376 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 228/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6512 - accuracy: 0.6198 - val_loss: 0.6440 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 229/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6250 - accuracy: 0.6446 - val_loss: 0.6399 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 230/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6424 - accuracy: 0.6612 - val_loss: 0.6538 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 231/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6575 - accuracy: 0.6612 - val_loss: 0.6467 - val_accuracy: 0.7000 - lr: 1.0000e-05\n",
            "Epoch 232/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6248 - accuracy: 0.6942 - val_loss: 0.6331 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 233/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6244 - accuracy: 0.6860 - val_loss: 0.6381 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 234/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6628 - accuracy: 0.6198 - val_loss: 0.6447 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 235/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6840 - accuracy: 0.5702 - val_loss: 0.6518 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 236/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.7002 - accuracy: 0.6446 - val_loss: 0.6598 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 237/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6759 - accuracy: 0.5785 - val_loss: 0.6433 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 238/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6900 - accuracy: 0.6198 - val_loss: 0.6626 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 239/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6645 - accuracy: 0.6446 - val_loss: 0.6522 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 240/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6571 - accuracy: 0.6116 - val_loss: 0.6494 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 241/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6233 - accuracy: 0.6694 - val_loss: 0.6531 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 242/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6742 - accuracy: 0.5785 - val_loss: 0.6346 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 243/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6451 - accuracy: 0.6529 - val_loss: 0.6325 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 244/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6692 - accuracy: 0.6446 - val_loss: 0.6509 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 245/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6496 - accuracy: 0.6364 - val_loss: 0.6420 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 246/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7016 - accuracy: 0.6033 - val_loss: 0.6518 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 247/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6594 - accuracy: 0.6694 - val_loss: 0.6389 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 248/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6656 - accuracy: 0.6529 - val_loss: 0.6382 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 249/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6815 - accuracy: 0.6364 - val_loss: 0.6395 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 250/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6778 - accuracy: 0.6198 - val_loss: 0.6520 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 251/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6404 - accuracy: 0.6446 - val_loss: 0.6340 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 252/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6679 - accuracy: 0.6281 - val_loss: 0.6335 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 253/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6259 - accuracy: 0.6694 - val_loss: 0.6482 - val_accuracy: 0.7000 - lr: 1.0000e-05\n",
            "Epoch 254/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6682 - accuracy: 0.6777 - val_loss: 0.6559 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 255/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6752 - accuracy: 0.6281 - val_loss: 0.6593 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 256/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6569 - accuracy: 0.6198 - val_loss: 0.6451 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 257/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6755 - accuracy: 0.6529 - val_loss: 0.6489 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 258/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6774 - accuracy: 0.6033 - val_loss: 0.6328 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 259/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6373 - accuracy: 0.6281 - val_loss: 0.6380 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 260/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6594 - accuracy: 0.6364 - val_loss: 0.6460 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 261/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6870 - accuracy: 0.6198 - val_loss: 0.6477 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 262/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6559 - accuracy: 0.6364 - val_loss: 0.6428 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 263/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6321 - accuracy: 0.6860 - val_loss: 0.6316 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 264/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6550 - accuracy: 0.6529 - val_loss: 0.6428 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 265/300\n",
            "31/31 [==============================] - 4s 129ms/step - loss: 0.6412 - accuracy: 0.6116 - val_loss: 0.6572 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 266/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6526 - accuracy: 0.6198 - val_loss: 0.6362 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 267/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6341 - accuracy: 0.6612 - val_loss: 0.6212 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 268/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6509 - accuracy: 0.6281 - val_loss: 0.6491 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 269/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6539 - accuracy: 0.6446 - val_loss: 0.6430 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 270/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6675 - accuracy: 0.6281 - val_loss: 0.6415 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 271/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6544 - accuracy: 0.6694 - val_loss: 0.6547 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 272/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.7221 - accuracy: 0.5950 - val_loss: 0.7126 - val_accuracy: 0.5667 - lr: 1.0000e-05\n",
            "Epoch 273/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6643 - accuracy: 0.6033 - val_loss: 0.6626 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 274/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6925 - accuracy: 0.6198 - val_loss: 0.6551 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 275/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6563 - accuracy: 0.5868 - val_loss: 0.6511 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 276/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6908 - accuracy: 0.6198 - val_loss: 0.6406 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 277/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6611 - accuracy: 0.6281 - val_loss: 0.6363 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 278/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6543 - accuracy: 0.6529 - val_loss: 0.6336 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 279/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6197 - accuracy: 0.6860 - val_loss: 0.6176 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 280/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6395 - accuracy: 0.6281 - val_loss: 0.6376 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 281/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6391 - accuracy: 0.6942 - val_loss: 0.6835 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 282/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6889 - accuracy: 0.5950 - val_loss: 0.6356 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 283/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6664 - accuracy: 0.6364 - val_loss: 0.6276 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 284/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6286 - accuracy: 0.6529 - val_loss: 0.6199 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 285/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6264 - accuracy: 0.7025 - val_loss: 0.6248 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 286/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6205 - accuracy: 0.6860 - val_loss: 0.6130 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 287/300\n",
            "31/31 [==============================] - 7s 243ms/step - loss: 0.6412 - accuracy: 0.6777 - val_loss: 0.6271 - val_accuracy: 0.7667 - lr: 1.0000e-05\n",
            "Epoch 288/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.7008 - accuracy: 0.6281 - val_loss: 0.6730 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 289/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6447 - accuracy: 0.6116 - val_loss: 0.6304 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 290/300\n",
            "31/31 [==============================] - 4s 129ms/step - loss: 0.6165 - accuracy: 0.6694 - val_loss: 0.6463 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 291/300\n",
            "31/31 [==============================] - 4s 130ms/step - loss: 0.6140 - accuracy: 0.6694 - val_loss: 0.6500 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 292/300\n",
            "31/31 [==============================] - 4s 129ms/step - loss: 0.6302 - accuracy: 0.6777 - val_loss: 0.6535 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 293/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6495 - accuracy: 0.6281 - val_loss: 0.6122 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 294/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6510 - accuracy: 0.6529 - val_loss: 0.6490 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 295/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6312 - accuracy: 0.5785 - val_loss: 0.6346 - val_accuracy: 0.6333 - lr: 1.0000e-05\n",
            "Epoch 296/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6607 - accuracy: 0.6198 - val_loss: 0.7109 - val_accuracy: 0.5333 - lr: 1.0000e-05\n",
            "Epoch 297/300\n",
            "31/31 [==============================] - 4s 128ms/step - loss: 0.6545 - accuracy: 0.6446 - val_loss: 0.6503 - val_accuracy: 0.7333 - lr: 1.0000e-05\n",
            "Epoch 298/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6325 - accuracy: 0.6777 - val_loss: 0.6349 - val_accuracy: 0.6667 - lr: 1.0000e-05\n",
            "Epoch 299/300\n",
            "31/31 [==============================] - 4s 127ms/step - loss: 0.6062 - accuracy: 0.6777 - val_loss: 0.6792 - val_accuracy: 0.6000 - lr: 1.0000e-05\n",
            "Epoch 300/300\n",
            "31/31 [==============================] - 4s 126ms/step - loss: 0.6674 - accuracy: 0.6364 - val_loss: 0.6670 - val_accuracy: 0.6333 - lr: 1.0000e-05\n"
          ]
        }
      ],
      "source": [
        "classifier.trainable = True\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
        "loss = 'categorical_crossentropy'\n",
        "#loss = tf.keras.losses.CategoricalHinge()\n",
        "metrics = ['accuracy']\n",
        "classifier.compile(\n",
        "    optimizer=optimizer, loss=loss, metrics=metrics#[\"accuracy\"]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='./model_checkpoint/modelv6.1',\n",
        "    save_weights_only=True,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True)\n",
        "\n",
        "\n",
        "\n",
        "my_callbacks = [\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.8,patience=5, min_lr=0.00001),\n",
        "    model_checkpoint_callback,\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "]\n",
        "\n",
        "epochs = 300  # @param {type: \"slider\", min:1, max:1000}\n",
        "hist = classifier.fit(casual_generator,\n",
        "                 validation_data = validation_generator,\n",
        "                 epochs=epochs,\n",
        "                 #steps_per_epoch = 150//4,\n",
        "                 callbacks=my_callbacks,\n",
        "                 verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AobYn8iukVFS"
      },
      "outputs": [],
      "source": [
        "print(model(x, training=False).numpy())\n",
        "\n",
        "print(model(x, training=True).numpy())\n",
        "print(y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}